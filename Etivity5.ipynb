{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Michel Danjou\n",
    "Student ID: 18263461"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The 'as' keyword allows you to invoke functionality from the module using an alias for the module name. For example: np.mean() instead of numpy.mean()\n",
    "- The from keyword allows you to only import the functionality of interest, for example above we import only the PCA class from the sklearn.decomposition module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import eig\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per E-tivity instructions: Use of the matrix class is discouraged, but to allow us to simplify the code slightly, we will use it this week. Its first use will be to store the data that you will perform the PCA transform on. Note that you will likely obtain a higher score if your final version does not use the matrix class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_x = 0.05\n",
    "a_y= 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data =  np.matrix([[n*(1+a_x*(rand.random()-0.5)),4*n+ a_y*(rand.random()-0.5)] for n in range(20)])\n",
    "data =  np.matrix([[n*(1+a_x*(rand.random()-0.5)),4*n+ a_y*(rand.random()-0.5)] for n in range(20)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numpy shape property is very useful to get an insight in the dimensions of your data, for example to check whether the features (in this case 2) or the samples (in this case 20) are in the rows or the columns. The notation used here (with columns containing the features and rows containing separate examples) is the standard for Scikitlearn and many other machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "[[ 0.          0.90027347]\n",
      " [ 1.01641417  4.25128047]\n",
      " [ 2.02009761  4.43578473]\n",
      " [ 3.07334389 16.38736042]\n",
      " [ 4.08633804 18.4369071 ]\n",
      " [ 4.96716178 20.31593515]\n",
      " [ 6.10474183 24.55645284]\n",
      " [ 6.99754441 30.45157999]\n",
      " [ 7.99416448 29.59323212]\n",
      " [ 8.79769288 34.14146389]\n",
      " [ 9.83238405 44.14282513]\n",
      " [11.25692529 40.25460452]\n",
      " [12.28203105 48.13302087]\n",
      " [13.16151529 54.45662961]\n",
      " [14.04005219 56.1888935 ]\n",
      " [15.31312356 61.62374677]\n",
      " [16.24011464 68.46335341]\n",
      " [17.38533923 64.57059066]\n",
      " [17.67275871 76.66855363]\n",
      " [19.29653998 74.95109812]]\n",
      "(20, 2)\n",
      "\n",
      "scikit.pca with nb_components=2\n",
      "eigen_values: [6.09893401e+02 5.35831452e-01]\n",
      "eigen_vectors: [[-0.24032714 -0.97069195]\n",
      " [ 0.97069195 -0.24032714]]\n",
      "\n",
      "scikit.pca with nb_components=1\n",
      "eigen_values: [609.8934007]\n",
      "eigen_vectors: [[-0.24032714 -0.97069195]]\n",
      "\n",
      "pca homebrew nb_components= 2\n",
      "eigen_values: [6.09893401e+02 5.35831452e-01]\n",
      "eigen_vectors: [[ 0.24032714 -0.97069195]\n",
      " [-0.97069195 -0.24032714]]\n",
      "\n",
      "pca homebrew nb_components= 1\n",
      "eigen_values: [609.8934007]\n",
      "eigen_vectors: [[ 0.24032714 -0.97069195]]\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import eig\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "        \n",
    "class Log:\n",
    "    DEBUG = 1\n",
    "    INFO = 2\n",
    "    ERROR = 3\n",
    "\n",
    "\n",
    "class My_pca:\n",
    "    \"\"\"\n",
    "    Perform the PCA on a dataset\n",
    "    \n",
    "    There is a lot of log statements in this class. I intend to remove\n",
    "    them in the final code. Leaving them in place for the time being as they\n",
    "    are useful for debugging. \n",
    "    \n",
    "    BUG ALERT\n",
    "    =========\n",
    "    The eigen values calculated by this class match the ones calculated by \n",
    "    scikit. \n",
    "    \n",
    "    However, it appears that one of the eigen vectors is the negative version \n",
    "    of the one calculted by scikit. Currently investigating the reason behind \n",
    "    this.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    log_level = Log.ERROR\n",
    "    nb_components = 2\n",
    "    eigen_values = []\n",
    "    eigen_vectors = []\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" init \"\"\"\n",
    "\n",
    "\n",
    "    def __log__(self, message, level=Log.INFO):\n",
    "        if level >= self.log_level:\n",
    "            print(message)\n",
    "\n",
    "        \n",
    "    def fit(self, matrix):\n",
    "        \"\"\" \n",
    "        Explicitly using array manipulation instead of \n",
    "        easier matrix operations \n",
    "        \"\"\"\n",
    "        self.matrix = matrix\n",
    "        \n",
    "        # Calculate mean values of each column from dataset\n",
    "        m0 = np.mean(self.matrix[:,0])\n",
    "        m1 = np.mean(self.matrix[:,1])\n",
    "        self.__log__((\"=\"*80))\n",
    "        self.__log__(\"mean.col0:{} \".format(m0))\n",
    "        self.__log__(\"mean.col1:{} \".format(m1))       \n",
    "        \n",
    "        # Center the columns by subtracting the corresponding mean\n",
    "        c0 = matrix[:,0] - m0\n",
    "        c1 = matrix[:,1] - m1\n",
    "        self.__log__(\"c0       :{} \".format(c0), Log.DEBUG)\n",
    "        self.__log__(\"c1       :{} \".format(c1), Log.DEBUG)       \n",
    "        \n",
    "        # Create a centered matrix \n",
    "        c_matrix = np.append(c0, c1, axis=1)\n",
    "        self.__log__(\"centered_matrix:\\n{}\".format(c_matrix), Log.DEBUG)\n",
    "        \n",
    "        # Calculate covariance of centered matrix\n",
    "        my_cov = np.cov(c_matrix, rowvar=False)        \n",
    "        self.__log__(\"covariance:\\n{}\".format(my_cov), Log.DEBUG)\n",
    "    \n",
    "        # eigen values, eigen vectors\n",
    "        eigen_values, eigen_vectors = eig(my_cov)\n",
    "        self.__log__(\"eigen_values:\\n{}\".format(eigen_values))\n",
    "        self.__log__(\"eigen_vectors:\\n{}\".format(eigen_vectors))     \n",
    "        \n",
    "        # order eigen values and eigen vectors       \n",
    "        sorted_eigen_values_indexes = eigen_values.argsort()[::-1]\n",
    "        sorted_eigen_values = eigen_values[sorted_eigen_values_indexes]\n",
    "        sorted_eigen_vectors = eigen_vectors[sorted_eigen_values_indexes] \n",
    "        self.__log__(\"sorted_eigen_values_indexes:\\n{}\".format(sorted_eigen_values_indexes))\n",
    "        self.__log__(\"sorted_eigen_values:\\n{}\".format(sorted_eigen_values))\n",
    "        self.__log__(\"sorted_eigen_vectors:\\n{}\".format(sorted_eigen_vectors))\n",
    "\n",
    "        # use nb_components to decide how many eigen vectors to keep\n",
    "        filtered_sorted_eigen_values = sorted_eigen_values[:self.nb_components]\n",
    "        filtered_sorted_eigen_vectors = sorted_eigen_vectors[:self.nb_components] \n",
    "        self.__log__(\"filtered_sorted_eigen_values:\\n{}\".format(sorted_eigen_values))\n",
    "        self.__log__(\"filtered_sorted_eigen_vectors:\\n{}\".format(sorted_eigen_vectors))\n",
    "        \n",
    "        # calculate projection of dataset onto the eigen vector basis\n",
    "        P = eigen_vectors.T.dot(c_matrix.T)        \n",
    "        self.__log__(\"projected  :\\n{}\".format(P.T), Log.DEBUG)\n",
    "        \n",
    "        # save results as class variables\n",
    "        self.eigen_values = filtered_sorted_eigen_values\n",
    "        self.eigen_vectors = filtered_sorted_eigen_vectors\n",
    "        self.projection = P\n",
    "    \n",
    "\n",
    "def build_dataset():\n",
    "    a_x = 0.05\n",
    "    a_y= 10\n",
    "\n",
    "    data =  np.matrix([[n*(1+a_x*(rand.random()-0.5)),4*n+ a_y*(rand.random()-0.5)] for n in range(20)])\n",
    "\n",
    "    print(data)\n",
    "    print(data.shape)\n",
    "    return data\n",
    "\n",
    "\n",
    "def scikit_pca( matrix, nb_components):\n",
    "    pca = PCA(nb_components)\n",
    "    pca.fit(matrix)\n",
    "    #print((\"=\"*80))\n",
    "    #print(\"pca.explained_variance_:      \\n{}\".format(pca.explained_variance_))\n",
    "    #print(\"pca.components_:              \\n{}\".format(pca.components_))\n",
    "    #print(\"pca.explained_variance_ratio_:\\n{}\".format(pca.explained_variance_ratio_), Log.DEBUG)\n",
    "    #print(\"pca.singular_values_:         \\n{}\".format(pca.singular_values_), Log.DEBUG)\n",
    "    #print(\"pca.mean_:                    \\n{}\".format(pca.mean_), Log.DEBUG)\n",
    "    #print(\"pca.n_components_:            \\n{}\".format(pca.n_components_), Log.DEBUG)\n",
    "    #print(\"pca.noise_variance_:          \\n{}\".format(pca.noise_variance_), Log.DEBUG)\n",
    "\n",
    "    return pca\n",
    "\n",
    "      \n",
    "def test():\n",
    "\n",
    "    my_pca = My_pca()\n",
    "\n",
    "    # Calculate PCA using scikit, nb_components=2\n",
    "    pca = scikit_pca(data, 2)\n",
    "    print()\n",
    "    print(\"scikit.pca with nb_components=2\")\n",
    "    print(\"eigen_values:\", pca.explained_variance_)\n",
    "    print(\"eigen_vectors:\", pca.components_)\n",
    "\n",
    "    # Calculate PCA using scikit, nb_components=1\n",
    "    pca = scikit_pca(data, 1)\n",
    "    print()\n",
    "    print(\"scikit.pca with nb_components=1\")\n",
    "    print(\"eigen_values:\", pca.explained_variance_)\n",
    "    print(\"eigen_vectors:\", pca.components_)\n",
    "\n",
    "    # Calculate PCA using homebrew code, nb_components=2\n",
    "    my_pca.nb_components=2\n",
    "    my_pca.fit(data)\n",
    "    print()\n",
    "    print(\"pca homebrew nb_components=\", my_pca.nb_components)\n",
    "    print(\"eigen_values:\", my_pca.eigen_values)\n",
    "    print(\"eigen_vectors:\", my_pca.eigen_vectors)\n",
    "    \n",
    "    # Calculate PCA using homebrew code, nb_components=1\n",
    "    my_pca.nb_components=1\n",
    "    my_pca.fit(data)\n",
    "    print()\n",
    "    print(\"pca homebrew nb_components=\", my_pca.nb_components)\n",
    "    print(\"eigen_values:\", my_pca.eigen_values)\n",
    "    print(\"eigen_vectors:\", my_pca.eigen_vectors)\n",
    "\n",
    "data = build_dataset()    \n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between Scikit and homebrew PCA\n",
    "## Scikit\n",
    "\n",
    "  * There are 2 eigen values: **580** and **0.45**. \n",
    "  * Because the 2nd eigen value is low we can deduct that the second eigen vector will not carry significant information and can be ignored.\n",
    "  * Looking at the values of the eigen vectors we can clearly see that they are **orthogonal**\n",
    "  * The first eigen vector is **[-0.24191617 -0.97029715]**\n",
    "\n",
    "```python\n",
    "scikit.pca with nb_components=2\n",
    "eigen_values: [5.80787811e+02 4.56934586e-01]\n",
    "eigen_vectors: [[-0.24191617 -0.97029715]\n",
    " [-0.97029715  0.24191617]]\n",
    "```\n",
    "## Homebrew implementation\n",
    "  * There are 2 eigen values: **580** and **0.45**. \n",
    "  * Because the 2nd eigen value is low we can deduct that the second eigen vector will not carry significant information and can be ignored.\n",
    "  * Looking at the values of the eigen vectors we can clearly see that they are **orthogonal**\n",
    "  * The first eigen vector is **[ 0.24191617 -0.97029715]**\n",
    "  \n",
    "```python\n",
    "pca homebrew nb_components= 2\n",
    "eigen_values: [5.80787811e+02 4.56934586e-01]\n",
    "eigen_vectors: [[ 0.24191617 -0.97029715]\n",
    " [-0.97029715 -0.24191617]]\n",
    "```\n",
    "`I note sign discrepencies between the eigen vectors returned by Scikit and the Homebrew implementation. The eigen basis is therefore different. I am looking for an explanation.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "scikit.pca with nb_components=2\n",
      "eigen_values: [6.09893401e+02 5.35831452e-01]\n",
      "eigen_vectors: [[-0.24032714 -0.97069195]\n",
      " [ 0.97069195 -0.24032714]]\n",
      "\n",
      "scikit.pca with nb_components=1\n",
      "eigen_values: [609.8934007]\n",
      "eigen_vectors: [[-0.24032714 -0.97069195]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate PCA using scikit, nb_components=2\n",
    "pca = scikit_pca(data, 2)\n",
    "print()\n",
    "print(\"scikit.pca with nb_components=2\")\n",
    "print(\"eigen_values:\", pca.explained_variance_)\n",
    "print(\"eigen_vectors:\", pca.components_)\n",
    "\n",
    "# Calculate PCA using scikit, nb_components=1\n",
    "pca = scikit_pca(data, 1)\n",
    "print()\n",
    "print(\"scikit.pca with nb_components=1\")\n",
    "print(\"eigen_values:\", pca.explained_variance_)\n",
    "print(\"eigen_vectors:\", pca.components_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between nb_component=1 and nb_component=2\n",
    "\n",
    "Since the value of the second eigen vector low, the second eigen vector can be ignored and thus the resulting eigen space can be simplified to **one dimension**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
