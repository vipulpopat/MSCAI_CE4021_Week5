{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Name: Brian Parle\n",
    "# Student ID:   18200036\n",
    "\n",
    "\n",
    "## Initial attempt: Sat Oct 13\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The purpose of this E-tivity is to help you become confident in using Numpyâ€™s functionality for matrix manipulation and to learn about a very useful data processing technique: principal components analysis, or PCA. You will find that using Numpy is much easier for matrix manipulation than what you have done to date!\n",
    "\n",
    "### Task (Complete by Saturday Week 5)\n",
    "\n",
    "Making use of Numpy, write a Python class to apply the PCA transform to the provided (see Notebook) data set. Compare the output of your implementation to the PCA functionality provided by the Scikitlearn module.\n",
    "\n",
    "1. Create a 'fit' method that calculates the eigen vectors and eigen values of your dataset. Compare your results to the output of Scikitlearn's fit method and document your findings as a comment (use markdown) directly under the cell with your PCA class.\n",
    "2. Use the Scikitlean's PCA class with n_components=2 and n_components=1 and observe the differences. In the cell directly below, comment on what you have observed.\n",
    "3. Add a property to your class and initialise this property in a suitable fashion to allow you to choose the number of principal components similar to the Scikitlearn PCA class.\n",
    "4. Store those results from your fit method that are required to transform the data set, in suitable class properties.\n",
    "5. Create a 'transform' method to perform the PCA data transformation on your data set using the parameters obtained using your 'fit' method.\n",
    " \n",
    "\n",
    "N.B.:\n",
    "\n",
    "* Limit your code to the aspects explicitly listed. \n",
    "* Use the Jupyter Notebook provided in the repository for week 5. This notebook contains the data that needs to be transformed.\n",
    "* The required modules have already been imported for you. You should not import any other modules.\n",
    "* If you find creating a class with this functionality daunting, please start by creating normal functions in your notebook. If time permits, you can then change to use of a class later. \n",
    " \n",
    "\n",
    "HINTS:\n",
    "\n",
    "- Numpy.mean() will 'flatten' your tensor by default. To obtain the mean along a given axis, you may use the axis parameter.\n",
    "- Numpy.cov() assumes by default that data is presented with one observation per column. This can be changed using the rowvar parameter. \n",
    "- A Numpy.matrix is a convenient way of performing the matrix operations required for PCA whilst retaining a matrix/vector like structure. Use of this class is discouraged, but would form a good starting point for tackling this week's challenge. Once you have the code working with the matrix class, changing to arrays is relatively straight forward.\n",
    "- You can use Scikitlearn as follows to check the Eigen vectors that you have found with your 'fit' mehod:\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(data)\n",
    "    print(pca.components_)\n",
    "\n",
    "You can use Scikitlearn to obtain \n",
    "GIT push your implementation and post your manual calculations to E-tivity 5: Linear Algebra in Numpy and Beyond and provide the name of your branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The 'as' keyword allows you to invoke functionality from the module using an alias for the module name. For example: np.mean() instead of numpy.mean()\n",
    "- The from keyword allows you to only import the functionality of interest, for example above we import only the PCA class from the sklearn.decomposition module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import eig\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per E-tivity instructions: Use of the matrix class is discouraged, but to allow us to simplify the code slightly, we will use it this week. Its first use will be to store the data that you will perform the PCA transform on. Note that you will likely obtain a higher score if your final version does not use the matrix class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_x = 0.05\n",
    "a_y= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  np.array([[n*(1+a_x*(rand.random()-0.5)),4*n+ a_y*(rand.random()-0.5)] for n in range(20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numpy shape property is very useful to get an insight in the dimensions of your data, for example to check whether the features (in this case 2) or the samples (in this case 20) are in the rows or the columns. The notation used here (with columns containing the features and rows containing separate examples) is the standard for Scikitlearn and many other machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          3.30649714]\n",
      " [ 1.02234573  2.62075349]\n",
      " [ 2.04865344 10.80992737]\n",
      " [ 3.0605403  13.91013541]\n",
      " [ 4.09314028 18.47385509]\n",
      " [ 4.96644467 18.71130212]\n",
      " [ 5.98963474 20.37603054]\n",
      " [ 6.87854018 31.54484907]\n",
      " [ 7.97167273 30.70933628]\n",
      " [ 9.08350935 35.81948019]\n",
      " [ 9.79118695 35.97582225]\n",
      " [11.03972883 40.50663654]\n",
      " [12.13771634 44.46381866]\n",
      " [13.31422563 48.9982566 ]\n",
      " [13.99430174 59.83105658]\n",
      " [14.7954468  60.33366355]\n",
      " [15.63051301 60.47909893]\n",
      " [16.89384245 66.23894532]\n",
      " [17.66758517 71.22500674]\n",
      " [19.41107512 79.13692456]]\n"
     ]
    }
   ],
   "source": [
    "print (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myPCA():\n",
    "    \n",
    "    def __init__(self, num_components):\n",
    "        \"\"\"\n",
    "        Set the number of components we want returned.\n",
    "        This value will be used in fit() to strip out the top num_components vectors\n",
    "        \"\"\"\n",
    "        self.n_components = num_components\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        provide access to the tensor's internal matrix\n",
    "        \"\"\"\n",
    "        return str(self.np_matrix)\n",
    "        \n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        The fit method needs to calculate the eigenvectors and eigenvalues of the data in matrix\n",
    "            #1. calc mean\n",
    "            #2. centre columns\n",
    "            #3. calc covariance\n",
    "            #4. calc eigenvalues and eigenvectors usnig numpy eig function\n",
    "            #5. calc the projection\n",
    "        \"\"\"   \n",
    "        self.np_matrix = data\n",
    "        self.shape = self.np_matrix.shape\n",
    "        #1. calc mean\n",
    "        self.np_mean = np.mean(self.np_matrix.T, axis=1).T\n",
    "        #2. centre\n",
    "        self.np_centre = self.np_matrix - self.np_mean\n",
    "        #3. calculate covariance\n",
    "        self.np_cov = np.cov(self.np_centre, rowvar=False)\n",
    "        #4. calculate eigenvectors and eigenvalues\n",
    "        self.np_eigenvalues, self.np_eigenvectors = eig(self.np_cov)\n",
    "        #- sort eigenvectors by absolute value of eigenvalues - ascending\n",
    "        array_sort = np.argsort(abs(self.np_eigenvalues))\n",
    "        # reverse sort order so index of largest eigenvector is first\n",
    "        array_sort = array_sort[::-1]\n",
    "        self.np_eigenvalues = self.np_eigenvalues[array_sort]\n",
    "        #transpose the eigenvectors as linalg returns the vectors in columns\n",
    "        self.np_eigenvectors = self.np_eigenvectors.T[array_sort]\n",
    "        # reduce to number of components specified\n",
    "        self.np_eigenvalues = self.np_eigenvalues[:self.n_components]\n",
    "        self.np_eigenvectors = self.np_eigenvectors[:self.n_components]\n",
    "        \n",
    "        return self.np_eigenvalues, self.np_eigenvectors\n",
    "    \n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        The transform function transforms the data using results from fit.\n",
    "        \"\"\"\n",
    "        #5. calculate the projection\n",
    "        data = data - self.np_mean\n",
    "        self.np_projection = self.np_eigenvectors.dot(data.T)\n",
    "        \n",
    "        #return the projection\n",
    "        return self.np_projection.T\n",
    "    \n",
    "    def inverse_transform(self, data):\n",
    "        \"\"\"\n",
    "        Return the Original Data Reduced with Mean added\n",
    "        \"\"\"\n",
    "        return np.dot(self.np_eigenvectors.T, self.np_projection).T + self.np_mean\n",
    "\n",
    "#\n",
    "#  ignore\n",
    "#\n",
    "#A = myPCA(2)\n",
    "#A.fit(data)\n",
    "#print (A.np_eigenvalues)\n",
    "#print (A.np_eigenvectors)\n",
    "#print (A.transform(data))\n",
    "#A.transform(data)\n",
    "#print (A.inverse_transform(data))\n",
    "\n",
    "#now calculate using scikit\n",
    "#pca = PCA(n_components=2)\n",
    "#pca.fit(data)\n",
    "#print(pca.explained_variance_)\n",
    "#print(pca.components_)\n",
    "#print(pca.transform(data))\n",
    "#pca.transform(data)\n",
    "#print(pca.inverse_transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.70291891e+02 4.30455976e-01]\n",
      "[[-0.24535793 -0.96943256]\n",
      " [-0.96943256  0.24535793]]\n",
      "[5.70291891e+02 4.30455976e-01]\n",
      "[[ 0.24535793  0.96943256]\n",
      " [-0.96943256  0.24535793]]\n"
     ]
    }
   ],
   "source": [
    "#Create a 'fit' method that calculates the eigen vectors and eigen values of your dataset. \n",
    "#Compare your results to the output of Scikitlearn's fit method and document your findings \n",
    "#as a comment (use markdown) directly under the cell with your PCA class.\n",
    "A = myPCA(2)\n",
    "A.fit(data)\n",
    "print (A.np_eigenvalues)\n",
    "print (A.np_eigenvectors)\n",
    "\n",
    "#now calculate using scikit\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(data)\n",
    "print(pca.explained_variance_)\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the output eigenvectors from my class and the scikit PCA:\n",
    "\n",
    "My Class: \n",
    "\n",
    "[[-0.24535793 -0.96943256]\n",
    " [-0.96943256  0.24535793]]\n",
    " \n",
    "scikit PCA:\n",
    "\n",
    "[[ 0.24535793  0.96943256]\n",
    " [-0.96943256  0.24535793]]\n",
    " \n",
    "The sign of the eigenvector is different, indicating that the vectors are pointing in opposite directions, but the magnitiudes are the same.\n",
    "\n",
    "These are the output eigenvalues frommy class and the scikit PCA:\n",
    "\n",
    "MyPCA [5.70291891e+02 4.30455976e-01]\n",
    "\n",
    "Scikit PCA: [5.70291891e+02 4.30455976e-01]\n",
    "\n",
    "The eigenvalues are exactly the same. The eigen values are sorted by size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* PCA Components: 2\n",
      "[[ 0.24535793  0.96943256]\n",
      " [-0.96943256  0.24535793]]\n",
      "[5.70291891e+02 4.30455976e-01]\n",
      "* PCA Components: 1\n",
      "[[0.24535793 0.96943256]]\n",
      "[570.29189123]\n"
     ]
    }
   ],
   "source": [
    "#2. Use the Scikitlearn's PCA class with n_components=2 and n_components=1 and observe the differences. \n",
    "#   In the cell directly below, comment on what you have observed.\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(data)\n",
    "print(\"* PCA Components: 2\")\n",
    "print(pca.components_)\n",
    "print(pca.explained_variance_)\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(data)\n",
    "print(\"* PCA Components: 1\")\n",
    "print(pca.components_)\n",
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With n_components=2, scikit has returned a 2x1 matrix of eigenvectors ordered by the eigenvalues of the vectors\n",
    "With n_components=1, scikit has returned a 1x2 matrix with only the eigenvector with the highest eigenvalue returned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reflect (Complete by Saturday Week 6)\n",
    "\n",
    "With your code (containing any corrections you have made based on your peersâ€™ feedback), do the following:\n",
    "\n",
    "1. For the case where n_components = 1, compare the resulting dataset of your transform method with the resulting dataset from Scikitlearnâ€™s transform method by plotting the points on an XY plot. If there are any differences, explain these in a comment directly under the cell with your plot.\n",
    "2. For the case where n_components = 1, compare the dataset resulting from your transform method with the original dataset by plotting the points on an XY plot. Comment on the differences between original and transformed data in the cell directly below your plot. In your comment, explain why and how PCA can be used for dimensionality reduction\n",
    " \n",
    "\n",
    "HINTS:\n",
    "\n",
    "You can use Scitkitlean as follows to calculate the new values of the data points in the original dataset when you reduce the dimensions of the data (from 2) to 1: \n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(data)\n",
    "data_pca = pca.transform(data)\n",
    "data_reduced = pca.inverse_transform(data_pca)\n",
    "\n",
    "You can use plots to compare the values in your original dataset with the dataset with reduced dimensionality:\n",
    "plt.plot(data[:,0], data[:,1], 'or')\n",
    "plt.plot(data_reduced[:,0], data_reduced[:,1],'xb')\n",
    "plt.show()\n",
    "\n",
    "You can use your own PCA results to calculate the new values of the data points int the original dataset when you reduce the dimensions of the data (from 2) to 1:\n",
    "reduced = np.dot(features[:,0],red_highvar.T)+mean.T\n",
    "\n",
    "with:\n",
    "- reduced: a 2x20 matrix of the new values of the dataset with dimensionality reduction applied\n",
    "- features[:,0]: the 2x1 matrix (or column vector) which contains the Eigen vector associated with the highest variance\n",
    "- red_highvar: a 20x1 matrix containing the reduced dataset which is the output of your transform method with n_components set to 1. \n",
    "- mean: a 1x2 matrix of the per-column mean values of your original data\n",
    "- T: the transform operator as provided by Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1. For the case where n_components = 1, compare the resulting dataset of your transform \n",
    "#method with the resulting dataset from Scikitlearnâ€™s transform method by plotting the \n",
    "#points on an XY plot. If there are any differences, explain these in a comment directly \n",
    "#under the cell with your plot.\n",
    "\n",
    "#Re-using the dataset above, with components=1, plot output from my class and Scikitlean\n",
    "pca = PCA(n_components=1) \n",
    "pca.fit(data) \n",
    "scikit_data_pca = pca.transform(data) \n",
    "scikit_data_reduced = pca.inverse_transform(scikit_data_pca)\n",
    "\n",
    "A = myPCA(1)\n",
    "A.fit(data)\n",
    "my_data_pca = A.transform(data)\n",
    "my_data_reduced = A.inverse_transform(data)\n",
    "\n",
    "#generate the plot\n",
    "plt.plot(my_data_reduced[:,0], my_data_reduced[:,1], 'or') \n",
    "plt.plot(scikit_data_reduced[:,0], scikit_data_reduced[:,1],'xb') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data from myPCA and the scikit PCA is (visually) identical as the data points from the 2 datasets correlate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#2. For the case where n_components = 1, compare the dataset resulting from your transform \n",
    "#method with the original dataset by plotting the points on an XY plot. Comment on the \n",
    "#differences between original and transformed data in the cell directly below your plot. \n",
    "#In your comment, explain why and how PCA can be used for dimensionality reduction.\n",
    "\n",
    "#Re-using the dataset above, with components=1, plot output from my class and Scikitlean\n",
    "pca = PCA(n_components=1) \n",
    "pca.fit(data) \n",
    "scikit_data_pca = pca.transform(data) \n",
    "scikit_data_reduced = pca.inverse_transform(scikit_data_pca)\n",
    "\n",
    "A = myPCA(1)\n",
    "A.fit(data)\n",
    "my_data_pca = A.transform(data)\n",
    "my_data_reduced = A.inverse_transform(data)\n",
    "\n",
    "#generate the plots\n",
    "plt.plot(data[:,0], data[:,1], 'or') \n",
    "plt.plot(my_data_reduced[:,0], my_data_reduced[:,1],'xb') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since only 1 component is being used, we are losing some data from the original dataset in the final transformed dataset. This loss is visually apparent as the data pooints form the 2 datasets are misaligned. The loss is equivalent to the sum of the ratios of the eliminated components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
